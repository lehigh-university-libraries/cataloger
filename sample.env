# LLM Provider Configuration
# Supported providers: openai, azure, gemini, ollama
CATALOGING_PROVIDER=ollama

# OpenAI Configuration
OPENAI_API_KEY=sk-proj-your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# Azure OpenAI Configuration
# AZURE_API_KEY=your-azure-api-key
# AZURE_ENDPOINT=https://your-instance.openai.azure.com
# AZURE_DEPLOYMENT=your-deployment-name
# AZURE_MODEL=gpt-4o

# Google Gemini Configuration
# GEMINI_API_KEY=your-gemini-api-key
# GEMINI_MODEL=gemini-pro-vision

# Ollama Configuration (for local models)
# Use OLLAMA_URL for remote instances, or OLLAMA_HOST for local
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=mistral-small3.2:24b

# Classification Models
EMBEDDING_MODEL=qwen-0.6b
VISION_MODEL=siglip

# OAI-PMH Configuration (for evaluation dataset fetching)
# OAI_PMH_URL=https://folio.example.edu/oai

# Image Fetching Configuration
# The eval tool fetches book images from:
# - Open Library Covers API (book covers)
# - Internet Archive (interior pages - title/copyright)
# - Google Books API (fallback for interior pages when IA not available)
# No API keys required - these are public APIs with rate limiting
# Rate limits:
#   - Open Library: 100 requests per 5 minutes per IP
#   - Google Books: No official limit, but we add 200ms delays between requests
